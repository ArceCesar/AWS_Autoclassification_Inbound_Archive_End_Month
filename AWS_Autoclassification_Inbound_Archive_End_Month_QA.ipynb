{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce7374c",
   "metadata": {},
   "source": [
    "# AWS_Autoclassification_Inbound_Archive_End_Month_QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e0198",
   "metadata": {},
   "source": [
    "### Create Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52146575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"agropur-global-nonprod-account-staging-sapco\"\n",
    "prefix = \"COPIA/Inbound/\"\n",
    "#prefix = \"SAPCO/Inbound/\"\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "prefixes = [prefix[\"Prefix\"] for prefix in response.get(\"CommonPrefixes\")]\n",
    "\n",
    "# Filtrar los archivos en cada subdirectorio\n",
    "filtered_files = []\n",
    "for prefix in prefixes:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    files = response.get(\"Contents\")\n",
    "    filtered_files.extend([file[\"Key\"] for file in files if file[\"Key\"].endswith(\".json.gz\") and \"_StagingDataLake_202306\" not in file[\"Key\"]])\n",
    "\n",
    "# Crear un DataFrame con los archivos filtrados\n",
    "df = pd.DataFrame(filtered_files, columns=[\"File\"])\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "excel_file = \"Files_QA_to_Archive.xlsx\"\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"File saved {excel_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329368fc",
   "metadata": {},
   "source": [
    "## AWS_S3_PROD_AutoClassification_Archive_byYear_ByMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def classify_file(source_bucket, source_key, target_bucket):\n",
    "    # Obtener el nombre del archivo y extraer el timestamp\n",
    "    file_name = source_key.split('/')[-1]\n",
    "    timestamp = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Verificar si el archivo contiene el timestamp deseado\n",
    "    if '202306' in timestamp:   # OJO Aqui deja de ejecutar el mes en curso\n",
    "        print(f\"File {source_key} do not be classified.\")\n",
    "        return\n",
    "\n",
    "    # Convertir el timestamp en un objeto de fecha y obtener el año y mes correspondientes\n",
    "    date = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    year = str(date.year)\n",
    "    month = date.strftime('%m-%b')  #('%m-%B') Number-Month  2-Feb.\n",
    "\n",
    "    # Construir la ruta de destino\n",
    "    target_key = source_key.replace('COPIA/Inbound', f'COPIA/Archive/{year}/{month}')\n",
    "\n",
    "    # Construir los comandos para mover el archivo\n",
    "    src_path = f's3://{source_bucket}/{source_key}'\n",
    "    tgt_path = f's3://{target_bucket}/{target_key}'\n",
    "    cmd = f'aws s3 mv {src_path} {tgt_path}'\n",
    "\n",
    "    # Mover el archivo al subdirectorio de destino\n",
    "    status = os.system(cmd)\n",
    "\n",
    "    if status == 0:\n",
    "        print(f'File {source_key} classified at {target_key}.')\n",
    "    else:\n",
    "        print(f'Error al transferir el archivo {source_key}.')\n",
    "\n",
    "def main():\n",
    "    # Definir los nombres de los cubos y la ruta de origen\n",
    "    source_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "    source_prefix = 'COPIA/Inbound/'\n",
    "    target_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "\n",
    "    # Leer el archivo Excel con la lista de archivos filtrados\n",
    "    excel_file = 'Files_QA_to_Archive.xlsx'\n",
    "    df = pd.read_excel(excel_file)\n",
    "    file_list = df['File'].tolist()\n",
    "\n",
    "    # Contador de archivos transferidos\n",
    "    transfer_count = 0\n",
    "\n",
    "    for file in file_list:\n",
    "        # Obtener la ruta de origen y clasificar el archivo\n",
    "        source_key = file\n",
    "        if classify_file(source_bucket, source_key, target_bucket):\n",
    "            transfer_count += 1\n",
    "    \n",
    "    print(f'Total files transfered: {transfer_count}')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c6915",
   "metadata": {},
   "source": [
    "Los dos en un solo script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59522e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def classify_file(source_bucket, source_key, target_bucket):\n",
    "    # Obtener el nombre del archivo y extraer el timestamp\n",
    "    file_name = source_key.split('/')[-1]\n",
    "    timestamp = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Verificar si el archivo contiene el timestamp deseado y si no contiene \"_StagingDataLake_202306\"\n",
    "    if '202307' in timestamp and '_StagingDataLake_202307' not in file_name:\n",
    "        print(f\"INFO: File {source_key} do not need to be classified.\")\n",
    "        return\n",
    "\n",
    "    # Convertir el timestamp en un objeto de fecha y obtener el año y mes correspondientes\n",
    "    date = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    year = str(date.year)\n",
    "    month = date.strftime('%m-%b')\n",
    "\n",
    "    # Construir la ruta de destino\n",
    "    target_key = source_key.replace('COPIA/Inbound', f'COPIA/Archive/{year}/{month}')\n",
    "    #target_key = source_key.replace('SAPCO/Inbound', f'SAPCO/Archive/{year}/{month}')\n",
    "\n",
    "    # Mover el archivo al subdirectorio de destino\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.copy_object(Bucket=target_bucket, Key=target_key, CopySource={'Bucket': source_bucket, 'Key': source_key})\n",
    "    s3_client.delete_object(Bucket=source_bucket, Key=source_key)\n",
    "\n",
    "    print(f'INFO: File {source_key} classified at {target_key}.')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Definir los nombres de los cubos y la ruta de origen\n",
    "    source_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "    source_prefix = 'COPIA/Inbound/'\n",
    "    #source_prefix = 'SAPCO/Inbound/'\n",
    "    target_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "\n",
    "    # Leer el archivo Excel con la lista de archivos filtrados\n",
    "    excel_file = 'Files_QA_to_Archive.xlsx'\n",
    "    df = pd.read_excel(excel_file)\n",
    "    file_list = df['File'].tolist()\n",
    "\n",
    "    # Contador de archivos transferidos\n",
    "    transfer_count = 0\n",
    "\n",
    "    for file in file_list:\n",
    "        # Obtener la ruta de origen y clasificar el archivo\n",
    "        source_key = file\n",
    "        classify_file(source_bucket, source_key, target_bucket)\n",
    "        transfer_count += 1\n",
    "\n",
    "    print(f'Total files transferred: {transfer_count}')\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Script executed successfully.'\n",
    "    }\n",
    "#agropur-global-prod-account-prod-sapco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d090e",
   "metadata": {},
   "source": [
    "## AWS LAMBDA Function (Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccf1fa",
   "metadata": {},
   "source": [
    "Fecha timestamp colocada de forma Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "\n",
    "def classify_file(source_bucket, source_key, target_bucket):\n",
    "    # Obtener el nombre del archivo y extraer el timestamp\n",
    "    file_name = source_key.split('/')[-1]\n",
    "    timestamp = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Verificar si el archivo contiene el timestamp deseado y si no contiene \"_StagingDataLake_202306\"\n",
    "    if not timestamp or ('202306' in timestamp and '_StagingDataLake_202306' not in file_name):\n",
    "        print(f\"INFO: File {source_key} do not need to be classified.\")\n",
    "        return\n",
    "    \n",
    "    # Convertir el timestamp en un objeto de fecha y obtener el año y mes correspondientes\n",
    "    date = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    year = str(date.year)\n",
    "    month = date.strftime('%m-%b')\n",
    "\n",
    "    # Construir la ruta de destino\n",
    "    target_key = source_key.replace('COPIA/Inbound', f'COPIA/Archive/{year}/{month}')\n",
    "    #target_key = source_key.replace('SAPCO/Inbound', f'SAPCO/Archive/{year}/{month}')\n",
    "\n",
    "    # Mover el archivo al subdirectorio de destino\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.copy_object(Bucket=target_bucket, Key=target_key, CopySource={'Bucket': source_bucket, 'Key': source_key})\n",
    "    s3_client.delete_object(Bucket=source_bucket, Key=source_key)\n",
    "\n",
    "    print(f'INFO: File {source_key} classified at {target_key}.')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Definir los nombres de los cubos y la ruta de origen\n",
    "    source_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "    source_prefix = 'COPIA/Inbound/'\n",
    "    target_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "\n",
    "    # Obtener la lista de archivos del bucket\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix, Delimiter='/')\n",
    "    files = response.get('Contents', [])\n",
    "\n",
    "    # Contador de archivos transferidos\n",
    "    transfer_count = 0\n",
    "\n",
    "    for file in files:\n",
    "        source_key = file['Key']\n",
    "        classify_file(source_bucket, source_key, target_bucket)\n",
    "        transfer_count += 1\n",
    "\n",
    "    print(f'Total files transferred: {transfer_count}')\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Script executed successfully.'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cec74",
   "metadata": {},
   "source": [
    "Obtener automaticamente timestamp del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238da790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "\n",
    "def classify_file(source_bucket, source_key, target_bucket, current_month):\n",
    "    # Obtener el nombre del archivo y extraer el timestamp\n",
    "    file_name = source_key.split('/')[-1]\n",
    "    timestamp = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Verificar si el archivo contiene el timestamp deseado y si no contiene \"_StagingDataLake_202306\"\n",
    "    if not timestamp or (current_month in timestamp and f\"_StagingDataLake_{current_month}\" not in file_name):\n",
    "        print(f\"INFO: File {source_key} does not need to be classified.\")\n",
    "        return\n",
    "    \n",
    "    # Convertir el timestamp en un objeto de fecha y obtener el año y mes correspondientes\n",
    "    date = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    year = str(date.year)\n",
    "    month = date.strftime('%m-%b')\n",
    "\n",
    "    # Construir la ruta de destino\n",
    "    target_key = source_key.replace('COPIA/Inbound', f'COPIA/Archive/{year}/{month}')\n",
    "    #target_key = source_key.replace('SAPCO/Inbound', f'SAPCO/Archive/{year}/{month}')\n",
    "\n",
    "    # Mover el archivo al subdirectorio de destino\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.copy_object(Bucket=target_bucket, Key=target_key, CopySource={'Bucket': source_bucket, 'Key': source_key})\n",
    "    s3_client.delete_object(Bucket=source_bucket, Key=source_key)\n",
    "\n",
    "    print(f'INFO: File {source_key} classified at {target_key}.')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    current_month = datetime.datetime.now().strftime(\"%Y%m\")\n",
    "#     file_name = event['file_name']\n",
    "#     timestamp = event['timestamp']\n",
    "    \n",
    "    # Definir los nombres de los cubos y la ruta de origen\n",
    "    source_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "    source_prefix = 'COPIA/Inbound/'\n",
    "    target_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "\n",
    "    # Obtener la lista de archivos del bucket\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix, Delimiter='/')\n",
    "    files = response.get('Contents', [])\n",
    "\n",
    "    # Contador de archivos transferidos\n",
    "    transfer_count = 0\n",
    "\n",
    "    for file in files:\n",
    "        source_key = file['Key']\n",
    "        classify_file(source_bucket, source_key, target_bucket, current_month)\n",
    "        transfer_count += 1\n",
    "\n",
    "    print(f'Total files transferred: {transfer_count}')\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Script executed successfully.'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240dd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "\n",
    "def classify_file(source_bucket, source_key, target_bucket, current_month):\n",
    "    # Obtener el nombre del archivo y extraer el timestamp\n",
    "    file_name = source_key.split('/')[-1]\n",
    "    timestamp = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "    # Verificar si el archivo contiene el timestamp deseado y si no contiene \"_StagingDataLake_202306\"\n",
    "    if not timestamp or (current_month in timestamp and f\"_StagingDataLake_{current_month}\" not in file_name):\n",
    "        print(f\"INFO: File {source_key} does not need to be classified.\")\n",
    "        return\n",
    "    \n",
    "    # Convertir el timestamp en un objeto de fecha y obtener el año y mes correspondientes\n",
    "    date = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "    year = str(date.year)\n",
    "    month = date.strftime('%m-%b')\n",
    "\n",
    "    # Construir la ruta de destino\n",
    "    target_key = source_key.replace('COPIA/Inbound', f'COPIA/Archive/{year}/{month}')\n",
    "    #target_key = source_key.replace('SAPCO/Inbound', f'SAPCO/Archive/{year}/{month}')\n",
    "\n",
    "    # Mover el archivo al subdirectorio de destino\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.copy_object(Bucket=target_bucket, Key=target_key, CopySource={'Bucket': source_bucket, 'Key': source_key})\n",
    "    s3_client.delete_object(Bucket=source_bucket, Key=source_key)\n",
    "\n",
    "    print(f'INFO: File {source_key} classified at {target_key}.')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    current_month = datetime.datetime.now().strftime(\"%Y%m\")\n",
    "    \n",
    "    # Definir los nombres de los cubos y la ruta de origen\n",
    "    source_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "    source_prefix = 'COPIA/Inbound/'\n",
    "    target_bucket = 'agropur-global-nonprod-account-staging-sapco'\n",
    "\n",
    "    # Obtener la lista de archivos del bucket\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix, Delimiter='/')\n",
    "    files = response.get('Contents', [])\n",
    "\n",
    "    # Contador de archivos transferidos\n",
    "    transfer_count = 0\n",
    "\n",
    "    for file in files:\n",
    "        source_key = file['Key']\n",
    "        classify_file(source_bucket, source_key, target_bucket, current_month)\n",
    "        transfer_count += 1\n",
    "\n",
    "    print(f'Total files transferred: {transfer_count}')\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Script executed successfully.'\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
